{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Import data\n",
        "The train and test split has already been done in the dataset, so I'll just import them from different csv files."
      ],
      "metadata": {
        "id": "ZJppdktkojlF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlgpAWimoYhX"
      },
      "outputs": [],
      "source": [
        "# load the zip file\n",
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile(\"world_happiness_competition_data.zip\", 'r') as zObject:\n",
        "    zObject.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "import pandas as pd\n",
        "X_train = pd.read_csv('world_happiness_competition_data/X_train.csv')\n",
        "X_test = pd.read_csv('world_happiness_competition_data/X_test.csv')\n",
        "y_train = pd.read_csv('world_happiness_competition_data/y_train.csv')\n",
        "y_train_labels = y_train.idxmax(axis=1)  # summarize y_train into one vector\n",
        "\n",
        "X_train.head()"
      ],
      "metadata": {
        "id": "wPXOFLqyosJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_labels.head()"
      ],
      "metadata": {
        "id": "TlZVLcAno7EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add new data"
      ],
      "metadata": {
        "id": "S7hgWFM9pJLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Truncated and cleaned up region data to merge (Week 4 folder)\n",
        "countrydata=pd.read_csv(\"newcountryvars.csv\")\n",
        "\n",
        "countrydata.head()"
      ],
      "metadata": {
        "id": "M_ZMROyTpJBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join new data to X_train and X_test by taking \"Country or region\" from first table and \"country_name\" from 2nd table.\n",
        "\n",
        "X_train = pd.merge(X_train, countrydata, how='left', left_on=[\"Country or region\"], right_on=[\"country_name\"])\n",
        "X_test= pd.merge(X_test, countrydata, how='left', left_on=[\"Country or region\"], right_on=[\"country_name\"])\n",
        "\n",
        "# only keep one key\n",
        "X_train.drop(columns=['country_name'], inplace=True)\n",
        "X_test.drop(columns=[\"country_name\"], inplace=True)"
      ],
      "metadata": {
        "id": "zAcVnMfxpMjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head(1)"
      ],
      "metadata": {
        "id": "EtReyEIrpUXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. EDA"
      ],
      "metadata": {
        "id": "yVq4Pl6Wpa2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# plot distribution of y variable\n",
        "happiness_order = ['Very Low', 'Low', 'Average', 'High', 'Very High']\n",
        "happiness_count = y_train_labels.value_counts()\n",
        "plt.bar(x=happiness_order, height=happiness_count[happiness_order])\n",
        "plt.title(\"Distributin of Happiness Level\")"
      ],
      "metadata": {
        "id": "NefmE6l-pZEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select columns from X_train and add y_trai\n",
        "df_try = X_train[[\"GDP per capita\", \"Freedom to make life choices\", \"hdi\", \"life_expectancy\", \"mean_years_of_schooling\"]]\n",
        "df_try[\"target\"] = y_train_labels\n",
        "\n",
        "# pairplot the relations between columns, the color indicates different happiness levels\n",
        "level_palette = [\"#ccdbdc\", \"#9ad1d4\", \"#80ced7\", \"#007ea7\", \"#003249\"]\n",
        "pairplot = sns.pairplot(df_try, hue='target', hue_order=happiness_order, palette=level_palette)\n",
        "pairplot._legend.set_title('Happiness Level')"
      ],
      "metadata": {
        "id": "nGALJ-Stprpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " With all the features that are intuitively considered to be positively correlated with happiness, the pairplot does roughly show this tendency.\n"
      ],
      "metadata": {
        "id": "zkl__QOXpxO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preprocessing"
      ],
      "metadata": {
        "id": "KmVgeRG4p75i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Create the preprocessing pipelines for both numeric and categorical data.\n",
        "numeric_features = X_train.drop([\"Country or region\", \"name\", \"region\", \"sub-region\"], axis=1)  ## Drop all the non-numerical features from X_train\n",
        "numeric_features = numeric_features.columns.tolist()\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "     ('imputer', SimpleImputer(strategy='constant', fill_value=0)), ## Is this good enough?\n",
        "     ('scaler', StandardScaler())]) # You will need to describe why this is being done in the next cell\n",
        "\n",
        "categorical_features = ['region', 'sub-region']\n",
        "\n",
        "# Replacing missing values with Modal value and then one hot encoding.\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown= 'error'))])\n",
        "\n",
        "# Final preprocessor object set up with ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "preprocess = preprocessor.fit(X_train)\n",
        "\n",
        "def preprocessor(data):\n",
        "    data.drop(['Country or region', 'name'], axis=1)\n",
        "    preprocessed_data=preprocess.transform(data)\n",
        "    return preprocessed_data"
      ],
      "metadata": {
        "id": "bcA6hCYgp6_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Establish the model"
      ],
      "metadata": {
        "id": "EHqpG7iLqUo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\": np.arange(50, 201, 50),\n",
        "    \"max_depth\": np.arange(5, 21, 5),\n",
        "    \"min_samples_split\": np.arange(2, 5),\n",
        "    \"min_samples_leaf\": np.arange(1, 5),\n",
        "    }\n",
        "\n",
        "gridmodel = GridSearchCV(model, param_grid, cv=5)\n",
        "\n",
        "gridmodel.fit(preprocessor(X_train), y_train_labels)\n",
        "\n",
        "print(\"best mean cross-validation score: {:.3f}\".format(gridmodel.best_score_))\n",
        "print(\"best parameters: {}\".format(gridmodel.best_params_))"
      ],
      "metadata": {
        "id": "4geJCZVWqJZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Save and Submit the model to aishare"
      ],
      "metadata": {
        "id": "GHEpxsCCqkXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set credentials using modelshare.org username/password\n",
        "import aimodelshare as ai\n",
        "\n",
        "apiurl=\"https://e2w6gh3id1.execute-api.us-east-2.amazonaws.com/prod/m\"\n",
        "ai.aws.set_credentials(apiurl=apiurl)"
      ],
      "metadata": {
        "id": "xLzWItj0rF4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate Competition\n",
        "mycompetition= ai.Competition(apiurl)"
      ],
      "metadata": {
        "id": "EkV0gNGfrOzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the preprocessor to aimodelshare\n",
        "ai.export_preprocessor(preprocessor,\"\")"
      ],
      "metadata": {
        "id": "zi6Vfxjxq2fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the RF model to local ONNX file\n",
        "feature_count = preprocessor(X_test).shape[1] #Get count of preprocessed features\n",
        "initial_type = [('float_input', FloatTensorType([None, feature_count]))] # Insert correct number of preprocessed features\n",
        "\n",
        "onnx_model = ai.aimsonnx.model_to_onnx(gridmodel, framework='sklearn',\n",
        "                          initial_types=initial_type,\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=False)\n",
        "\n",
        "with open(\"model.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "EjVaVKJlqcD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predicted values\n",
        "prediction_labels = gridmodel.predict(preprocessor(X_test))\n",
        "gridmodel_filepath = \"model.onnx\"\n",
        "preprocessor_filepath=\"preprocessor.zip\"\n",
        "\n",
        "# Submit to Competition Leaderboard\n",
        "mycompetition.submit_model(model=gridmodel_filepath,\n",
        "                           prediction_submission=prediction_labels,\n",
        "                           preprocessor=preprocessor_filepath)"
      ],
      "metadata": {
        "id": "gEbPJa1_qoTx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}